{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRL via Apprenticeship Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- https://github.com/Farama-Foundation/HighwayEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "from matplotlib import pyplot as plt\n",
    "import pprint\n",
    "import numpy as np\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the highway environment with configurations\n",
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"TimeToCollision\",\n",
    "        \"horizon\": 10\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "        \"longitudinal\": True,\n",
    "        \"lateral\": True\n",
    "    },\n",
    "    \"duration\": 40,\n",
    "    \"lanes_count\": 2,\n",
    "    \"vehicles_density\": 1.0,\n",
    "    \"collision_reward\": -1,\n",
    "    \"right_lane_reward\": 0.1,\n",
    "    \"high_speed_reward\": 0.4,\n",
    "    \"reward_speed_range\": [20, 30],\n",
    "    \"normalize_reward\": True\n",
    "}\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='rgb_array', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': {'lateral': True,\n",
      "            'longitudinal': True,\n",
      "            'type': 'DiscreteMetaAction'},\n",
      " 'centering_position': [0.3, 0.5],\n",
      " 'collision_reward': -1,\n",
      " 'controlled_vehicles': 1,\n",
      " 'duration': 40,\n",
      " 'ego_spacing': 2,\n",
      " 'high_speed_reward': 0.4,\n",
      " 'initial_lane_id': None,\n",
      " 'lane_change_reward': 0,\n",
      " 'lanes_count': 2,\n",
      " 'manual_control': False,\n",
      " 'normalize_reward': True,\n",
      " 'observation': {'horizon': 10, 'type': 'TimeToCollision'},\n",
      " 'offroad_terminal': False,\n",
      " 'offscreen_rendering': False,\n",
      " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
      " 'policy_frequency': 1,\n",
      " 'real_time_rendering': False,\n",
      " 'render_agent': True,\n",
      " 'reward_speed_range': [20, 30],\n",
      " 'right_lane_reward': 0.1,\n",
      " 'scaling': 5.5,\n",
      " 'screen_height': 150,\n",
      " 'screen_width': 600,\n",
      " 'show_trajectories': False,\n",
      " 'simulation_frequency': 15,\n",
      " 'vehicles_count': 50,\n",
      " 'vehicles_density': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(env.unwrapped.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to finite MDP\n",
    "env.reset()\n",
    "base_env = env.unwrapped\n",
    "mdp = base_env.to_finite_mdp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP -> Constrained MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = importlib.import_module(\"finite_mdp.mdp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_mdp = module.StochasticMDP.from_deterministic(mdp)\n",
    "\n",
    "# Now create the cost matrix\n",
    "num_states = mdp.transition.shape[0]\n",
    "num_actions = mdp.transition.shape[1]\n",
    "\n",
    "# Define cost matrix (S x A)\n",
    "cost_matrix = np.zeros((num_states, num_actions))\n",
    "# Example costs based on actions:\n",
    "cost_matrix[:, 0] = 0.8  # Lane left\n",
    "cost_matrix[:, 1] = 0.1  # Idle\n",
    "cost_matrix[:, 2] = 0.4  # Lane right\n",
    "cost_matrix[:, 3] = 0.2  # Faster\n",
    "cost_matrix[:, 4] = 0.2  # Slower\n",
    "\n",
    "# Create constrained MDP from stochastic MDP\n",
    "constrained_mdp = module.ConstrainedMDP(\n",
    "    transition=stochastic_mdp.transition,\n",
    "    reward=stochastic_mdp.reward,\n",
    "    cost=cost_matrix,\n",
    "    terminal=stochastic_mdp.terminal\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1, Reward: 0.00, Cost: 0.10\n",
      "Action: 3, Reward: 0.00, Cost: 0.30\n",
      "Action: 0, Reward: 0.00, Cost: 0.80\n",
      "Action: 4, Reward: 0.00, Cost: 0.30\n",
      "Action: 2, Reward: 0.00, Cost: 0.80\n"
     ]
    }
   ],
   "source": [
    "state = constrained_mdp.reset()\n",
    "for _ in range(5):\n",
    "    action = np.random.randint(0, num_actions)\n",
    "    next_state, reward, done, info = constrained_mdp.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward:.2f}, Cost: {info['c_']:.2f}\")\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 1.0, (3, 3, 10), float32)\n"
     ]
    }
   ],
   "source": [
    "# print the observation space\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Num | Observation Dimension | Meaning | Length | Min | Max |\n",
    "|-----|------------------------|---------|---------|-----|-----|\n",
    "| 0 | Ego‐speed channels | Predictions at V discrete ego‐speeds (e.g. low/med/high m/s) | 3 | 0.0 | 1.0 |\n",
    "| 1 | Lane channels | L lanes around the ego‐vehicle (left, current, right) | 3 | 0.0 | 1.0 |\n",
    "| 2 | Time-to-collision bins (horizon) | H discretized future time steps for collision prediction | 10 | 0.0 | 1.0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "# print the action space\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type: Discrete(5)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Lane Left\n",
    "1 | Idle\n",
    "2 | Lane Right\n",
    "3 | Faster\n",
    "4 | Slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LANE_LEFT', 1: 'IDLE', 2: 'LANE_RIGHT', 3: 'FASTER', 4: 'SLOWER'}\n"
     ]
    }
   ],
   "source": [
    "# Get all possible actions\n",
    "ACTIONS_ALL = base_env.action_type.actions\n",
    "print(ACTIONS_ALL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward function is usually composed of a velocity term and a collision term:\n",
    "\n",
    "$R(s,a) = a\\frac{v-v_{min}}{v_{max}-v_{min}} - b*\\text{collision}$\n",
    "\n",
    "where $v$, $v_{min}$, $v_{max}$ are the current, minimum and maximum speed of the ego-vehicle respectively, and $a$, $b$ are two coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Reward Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In finite case, I redefine reward function as \n",
    "\n",
    "$r(s,a) = r(s) + r(a)$\n",
    "\n",
    "where r(s) is the reward for the state and r(a) is the reward for the action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Expert Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "from rl_agents.agents.common.factory import agent_factory\n",
    "#from utils import record_videos, show_videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create value iteration agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "agent_factory() got an unexpected keyword argument 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/joewagner/Documents/GitHub/2025Spring/IRL_driving_environment/IRL_driving_environment/rl_scripts/apprenticeship_learning.ipynb Cell 30\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joewagner/Documents/GitHub/2025Spring/IRL_driving_environment/IRL_driving_environment/rl_scripts/apprenticeship_learning.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mrl_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfactory\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m agent_factory\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/joewagner/Documents/GitHub/2025Spring/IRL_driving_environment/IRL_driving_environment/rl_scripts/apprenticeship_learning.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m agent \u001b[39m=\u001b[39m agent_factory(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joewagner/Documents/GitHub/2025Spring/IRL_driving_environment/IRL_driving_environment/rl_scripts/apprenticeship_learning.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joewagner/Documents/GitHub/2025Spring/IRL_driving_environment/IRL_driving_environment/rl_scripts/apprenticeship_learning.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     env\u001b[39m=\u001b[39;49mconstrained_mdp,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joewagner/Documents/GitHub/2025Spring/IRL_driving_environment/IRL_driving_environment/rl_scripts/apprenticeship_learning.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: agent_factory() got an unexpected keyword argument 'env'"
     ]
    }
   ],
   "source": [
    "from rl_agents.agents.common.factory import agent_factory\n",
    "\n",
    "agent = agent_factory(constrained_mdp, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
